**  latency # seconds / # trips
**  bandwidth # bytes sent and recieved / # seconds

*** Internship Sandia ***

* Hello, my name is Keira Haskins. I am a PhD student working with Patrick Bridges at
  UNM. To start off working at Sandia, I will be working on using nvprof or 
  similar tools to analyze the performance and communication patterns of applications 
  which run on the GPU. Recently

  I have been spending time working on an MPI/CUDA based ping pong test which focuses
  on three different techniques; direct sending and recieving,
  a cuda aware method, and a method for copying data to the GPU first. From here I will
  use nvprof to analyze various methods for communication between the CPU and
  GPU on this application before moving on to larger applications, possibly FIESTA
  or HIGRAD; As an example for how we may change our communication patterns; should 
  we call pack send pack send, maybe pack pack send send, or possibly other patterns? 
  I ultimately want to be able to analyze GPU application communication patterns
  in order to determine what could be changed to improve GPU application performance.

* I've gotten my ping pong test working and I've started running it with nvprof, and
  I have since started looking at where it's spending most of its time for different
  methods.

* Ping pong test -> started working on making it more dynamic, such that I can adjust
  different parameters in it; which direction we are accessing data structures, how many
  dimensions we are looking at etc

* I've started looking at nvprof output for the ping pong test and I've started working
  on how to best make graphs of the data for bandwidth, latency, and runtime along with
  the data relating to where time is being spent in the application as output from
  nvprof.

* I've also started putting together a presentation for a meeting with Carl Pearson,
  but otherwise I don't have much more to discuss.

*** Additional Data ***

* Direct
640,8.326584,0.004163,11806041701.367895

* Cuda aware
640,11.735221,0.005868,8376834419.358094
640,11.853410,0.005927,8293309709.931513

* Copy
640,27.059686,0.013530,3632858096.816575

*** Additional Code ***

      //leftRecvStarts[0]   = 0;
      //leftRecvStarts[1]   = 0;
      //leftRecvStarts[2]   = 0;
      //leftRecvStarts[3]   = 0;

      //leftSendStarts[0]   = cf.ng;
      //leftSendStarts[1]   = 0;
      //leftSendStarts[2]   = 0;
      //leftSendStarts[3]   = 0;

      //rightRecvStarts[0]  = cf.ngi - cf.ng;
      //rightRecvStarts[1]  = 0;
      //rightRecvStarts[2]  = 0;
      //rightRecvStarts[3]  = 0;

      //rightSendStarts[0]  = cf.nci;
      //rightSendStarts[1]  = 0;
      //rightSendStarts[2]  = 0;
      //rightSendStarts[3]  = 0;

      //leftRecvStarts[0]   = 0;
      //leftRecvStarts[1]   = 0;
      //leftRecvStarts[2]   = 0;
      //leftRecvStarts[3]   = 0;

      //leftSendStarts[0]   = 0;
      //leftSendStarts[1]   = cf.ng;
      //leftSendStarts[2]   = 0;
      //leftSendStarts[3]   = 0;

      //rightRecvStarts[0]  = 0;
      //rightRecvStarts[1]  = cf.ngj - cf.ng;
      //rightRecvStarts[2]  = 0;
      //rightRecvStarts[3]  = 0;

      //rightSendStarts[0]  = 0;
      //rightSendStarts[1]  = cf.ncj;
      //rightSendStarts[2]  = 0;
      //rightSendStarts[3]  = 0;

      //leftRecvStarts[0]   = 0;
      //leftRecvStarts[1]   = 0;
      //leftRecvStarts[2]   = 0;
      //leftRecvStarts[3]   = 0;

      //leftSendStarts[0]   = 0;
      //leftSendStarts[1]   = 0;
      //leftSendStarts[2]   = cf.ng;
      //leftSendStarts[3]   = 0;

      //rightRecvStarts[0]  = 0;
      //rightRecvStarts[1]  = 0;
      //rightRecvStarts[2]  = cf.ngk - cf.ng;
      //rightRecvStarts[3]  = 0;

      //rightSendStarts[0]  = 0;
      //rightSendStarts[1]  = 0;
      //rightSendStarts[2]  = cf.nck;
      //rightSendStarts[3]  = 0;

*** Posters ***

  * 13th August / 17/18 days

*** Presentation Ping Pong ***

working with Patrick Bridges at
UNM, and Kurt Ferreira and Scott Levy at Sandia.

Slide 1:
  Hello! My name is Rei Haskins, I am a grad student working at UNM with Patrick 
  Bridges, and at Sandia with Kurt Ferreira and Scott Levy, and today I will be
  discussing the work that we have been doing on the analysis of HPC applications
  which run on GPUs using MPI.

Slide 2:
  So to start,
  Our motivation for this work is to first determine if changes made to communication
  patterns and how data is packed onto GPUs may improve overall application
  performance. Secondly, as a whole we would like to be able to learn more about
  communication and data movement within HPC applications.

Slide 3:
  First I would like to talk about our desire to analyze GPU applications. To start,
  we created an MPI library to profile GPU activities during communication of HPC
  applications. Second, we built a ping pong test which uses a number of Kokkos features
  borrowed from FIESTA. We designed it such that it uses real application data structures
  and MPI types.

Slide 4:
  For our ping pong test we extracted data structures from FIESTA. The
  ping pong test supports three different communication methods: First, there is
  direct communication which sends and receives directly from GPU to GPU without any
  data copying first. Second, we have a cuda aware method, which first packs data into
  a flat Kokkos buffer before sending and receiving between GPUs. Lastly, we have a
  copy method which starts off by performing a deep copy, or the moving data from GPU 
  to the host before sending and receiving between GPUs.

Slide 5:
  Next I would like to present some results that I have gathered from running the ping
  pong test with nvprof on Lassen. To start, the first three top plots are the duration, 
  latency, and bandwidth produced by the application itself. Below each one is the 
  corresponding output from nvprof showing where each run was spending most of its 
  time. 
  As we can see for the direct method around 50 percent of the application time 
  is being spent in
  CUDA memcopy from device to host and another 48 percent is being spent copying memory
  for CUDA from host to device. 

  Looking at the first figure we may also observe that overall the direct method gets
  better performance than either the cuda aware or the copy methods. Note that this is
  using C ordering for Kokkos.

  #the bandwidth is higher than for both cuda aware and copy and the duration and latency
  #is lower than both cuda aware and copy and that cuda aware is also better than copy
  #in each area as well. Also of note, both cuda aware and copy spend about 33 percent
  #in cuda_parallel_launch_local_memory twice before then spending around 15-16 percent
  #of their time in CUDA memcopy from device to host and then host to device. 

  Max_i is
  used to define one side of the data structure being used; for this we looked at 10,
  20, 40, 80, 160, 320, and 640. Also of note, we ran these tests using C order data
  structure accessing. When we switch over to Fortran ordering we get around 40 seconds
  for a max_i of 10 for the direct method whereas both CUDA aware and copy stay at about
  the same performance.

Slide 6:
  Next I would like to show some results for the direct method when changing the 
  direction of accessing data in either the x, y, or z dimensions. The first figure
  is the same as the direct method from the last slide and we may observe the higher
  bandwidth and the NVprof output wherein there are only 24 calls to CUDA memcopy. In
  the y and z directions the latencies are much higher, especially in the z direction
  and the bandwidth for each are very low. We may also observe that for y there are
  around 80,000 calls to CUDA memcopy and over 3,000,000 for the z direction. 

Slide 7:
  Now, I would like to talk a bit about where we would like to go next: First we would
  like to modify the ping pong test in order to make use of dimensions other than x and
  we want to use Fortran order arrays. Second, we want to look at using other versions of
  MPI in order to see how GPU usage changes depending on the version. Third, we want to
  see about running on different systems and or accelerators. In the long term, we want
  to look at running on non-NVIDIA accelerators, AMD, etc, and lastly we want to consider
  integration with other performance monitoring systems other than NVprof.

Slide 8:
  Lastly I would like to acknowledge ...

Slide 9:
  Thank you for listening to my talk, are there any questions?


*** Ping Pong Notes ***
    ** Talk to Ryan and or Amanda about compiling with Mvapich.
    
    * Have a graph of the pieces of code -> Packing in one direction is a node, sending,
      receiving, unpacking, are all nodes; each node is a snippet of code.
    * Generate a ping pong that works in a particular order based on a 
    * Evaluate a few cases by hand to start
    ** Take FIESTA and make an alternate communication strategy -> Ask Ryan about this
      - Ordering of packing and sending
    * Want to understand what's going on between all packs/sends vs alternating

*** Ping Pong Data ***

,0.078896
,0.087249
,0.118332
,0.224270
,0.698187
,2.238316
,8.326584


,0.118801
,0.135347
,0.163165
,0.263389
,0.894650
,2.877888
,11.85341


,0.185193
,0.209339
,0.300820
,0.545074
,1.696082
,6.218402
,27.05968

direct x
320,0.001078,11402078314.659918
640,0.004027,12204511793.726007

direct y
320,0.017196,714601616.617975
640,0.050782,967900745.201599

cuda x
320,0.001414,8688246751.909416
640,0.005845,8409797301.958844

cuda y
320,2.724455,0.001362,9020518425.556820,1
640,11.102240,0.005551,8854429511.995148,1

cuda z
320,2.722141,0.001361,9028188143.611574,2
640,11.247306,0.005624,8740226486.279173,2

copy x
320,0.003069,4003636486.599613
640,0.013575,3620798813.318490

copy y
320,6.058765,0.003029,4056272018.010347,1
640,26.555536,0.013278,3701827056.404373,1

copy z
320,6.032429,0.003016,4073980577.683510,2
640,26.376700,0.013188,3726925656.883216,2


*** CUDA Streams and Interface ***

  * gpudirect async
  * libmp
  * MPI_Send_Init -> MPI_Start(&reqs[0])
  * MPI_Stream_Enqueue(&stream, &reqs[0])

  * Sketch out stream triggered communication and look at persistent communication.
  * cudaGraphAddMemcpyNode
  * Long message protocol -> sending long messages
  * Clear to send CTS


* My work focuses on the performance analysis of hybrid GPU/MPI applications
  running on large scale High Performance Computing systems. Currently I have 
  been working on building a ping pong test
  which utilizes Kokkos data structures and primitives borrowed from the particle and
  atmospheric code FIESTA and adapted to my test. The ping pong uses a four dimensional
  array; it has three spatial dimensions and a fourth for variables. Ultimately I have
  been focused on building a program which tests real world data structures using MPI
  and GPU communication between nodes using various techniques and it may be configured
  to access data in various directions. The tool implements three different methods of
  GPU data transfer; directly sending and receiving between nodes, a cuda aware method
  which first packs data into a flat Kokkos buffer and a copy method which uses a deep
  copy to move data from device to host before sending and receiving I have also been
  using code for wrapping MPI
  calls which may then be monitored by NVprof, the NVIDIA profiling library.
  Ultimately I want to develop a framework for analyzing
  such applications. Next steps include making use of Fortran order arrays, using
  different versions of MPI to see how GPU usage varies, implementing stream-triggered
  communication, and implementing persistent communication. Lastly I want to look 
  into making use of profiling tools other than NVprof

** Meeting CS 600 -> Friday 1:00 pm 3rd Aug.

This is what we think we need, here is how we have gone about it so far

** Requirements
* We need benchmarks which are more representative of how we run MPI
  ** We need them to be more tightly coupled with GPU monitoring tools
  *  Where are we in the process (paragraph 2)
  * To what degree is that design or setting up contributions

  * Contributions:

 * We did this for this reason *
 * * We need to talk about the "why", why did we do this?? * *

 * 03/10/2022 *

  * Overlay library using Nickel
  * Similar to Amanda's work
  * Construct CUDA graphs, hand them to the GPU and say go.

* 06/01/2022 *
  * Use a postal model to measure alpha and beta in our benchmark
  * Read over Amanda's paper and think about fitting a postal model
    to the various outputs of our benchmark
  * Run with small changes in n, then convert to bytes; table -> size vs latency, 
    then fit a linear regression to it. the intercept is alpha and the slope is
    beta.
  * Look for papers that model GPU launch times; kernel launch and synchronization times
    
